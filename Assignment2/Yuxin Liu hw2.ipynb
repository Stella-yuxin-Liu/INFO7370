{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0491d809",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Deep Q Learning on Atari's Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14441ff",
   "metadata": {},
   "source": [
    "## Setting Environment\n",
    "We use OpenAI's gym environment. The Pong's ROM was provided by AutoROM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81e8076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e61c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14983d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (210, 160, 3), uint8)\n",
      "Discrete(6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Application\\Anaconda\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v5` with the environment ID `ALE/Pong-v5`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# liu\n",
    "# import game rom\n",
    "env = gym.make('Pong-v0')\n",
    "observation = env.reset()\n",
    "# status space\n",
    "print(env.observation_space)\n",
    "# actions space\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092835f",
   "metadata": {},
   "source": [
    "## Process the Frames\n",
    "The third element of the observation is the image frame of the game. It contains size and channels. Before any further steps, we processed the image to gray graphic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8646d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(new_frame,last_frame):\n",
    "    # inputs are 2 numpy 2d arrays\n",
    "    n_frame = new_frame.astype(np.int32)\n",
    "    n_frame[(n_frame==144)|(n_frame==109)]=0 # remove backgound colors\n",
    "    l_frame = last_frame.astype(np.int32)\n",
    "    l_frame[(l_frame==144)|(l_frame==109)]=0 # remove backgound colors\n",
    "    diff = n_frame - l_frame\n",
    "    # crop top and bot \n",
    "    diff = diff[35:195]\n",
    "    # down sample \n",
    "    diff=diff[::2,::2]\n",
    "    # convert to grayscale\n",
    "    diff = diff[:,:,0] * 299. / 1000 + diff[:,:,1] * 587. / 1000 + diff[:,:,2] * 114. / 1000\n",
    "    # rescale numbers between 0 and 1\n",
    "    max_val =diff.max() if diff.max()> abs(diff.min()) else abs(diff.min())\n",
    "    if max_val != 0:\n",
    "        diff=diff/max_val\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32cbf7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e9d64a94c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    observation, reward, done, info = env.step(0)# 0 means stay the same place(or do nothing)\n",
    "new_observation, reward, done, info = env.step(2)\n",
    "plt.imshow(preprocess_frames(new_observation,observation),plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2904ca61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_frames(new_observation,observation).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2e764",
   "metadata": {},
   "source": [
    "The processed frame is 80 * 80."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350f4bf",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "Using a simple 2 layer model with 200 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "208a9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 80, 80)]          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               1280000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 200       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,200\n",
      "Trainable params: 1,280,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(state_shape, action_shape):\n",
    "    inputs = keras.layers.Input(shape=(80,80))\n",
    "    flattened_layer = keras.layers.Flatten()(inputs)\n",
    "    full_connect_1 = keras.layers.Dense(units=200,activation='relu',use_bias=False,)(flattened_layer)\n",
    "    sigmoid_output = keras.layers.Dense(1,activation='sigmoid',use_bias=False)(full_connect_1)\n",
    "    policy_network_model = keras.models.Model(inputs=inputs,outputs=sigmoid_output)\n",
    "    policy_network_model.summary()\n",
    "    return policy_network_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135fd39",
   "metadata": {},
   "source": [
    "The model has about 1.28 million parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc7115",
   "metadata": {},
   "source": [
    "## Trainning\n",
    "Training episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a3466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, replay_memory, model, target_model, done):\n",
    "    learning_rate = 0.8\n",
    "    discount_factor = 0.8\n",
    "\n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "\n",
    "    batch_size = 10\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    current_states = np.array([transition[0] for transition in mini_batch])\n",
    "    current_qs_list = model.predict(current_states)\n",
    "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
    "    future_qs_list = target_model.predict(new_current_states)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "\n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71785ade",
   "metadata": {},
   "source": [
    "Because of the time and memory limit, the model is only trained for 100 episodes. In order to get workable result, the trainning episodes shall be more than 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_times = 100 # shall be more than 10000\n",
    "batch_size = 10\n",
    "learning_rate = 0.8 # 0.7\n",
    "epsilon = 1.\n",
    "max_epsilon = 1.\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.01 # 0.05\n",
    "\n",
    "model = create_model(env.observation_space.shape, env.action_space.n)\n",
    "target_model = create_model(env.observation_space.shape, env.action_space.n)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "target_update_counter = 0\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "steps_to_update_target_model = 0\n",
    "for episode in range(train_episodes):\n",
    "    total_training_rewards = 0\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        steps_to_update_target_model += 1\n",
    "\n",
    "        random_number = np.random.rand()\n",
    "        if random_number <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            encoded = observation\n",
    "            encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "            predicted = model.predict(encoded_reshaped).flatten()\n",
    "            action = np.argmax(predicted)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        replay_memory.append([observation, action, reward, new_observation, done])\n",
    "        if steps_to_update_target_model % 4 == 0 or done:\n",
    "            train(env, replay_memory, model, target_model, done)\n",
    "\n",
    "        observation = new_observation\n",
    "        total_training_rewards += reward\n",
    "\n",
    "        if done:\n",
    "            total_training_rewards += 1\n",
    "\n",
    "            if steps_to_update_target_model >= 100:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                steps_to_update_target_model = 0\n",
    "            break\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0405cae",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Test the trained model against traditional computer. Display reward for every game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode(policy_network):\n",
    "    env = gym.make('Pong-v0')\n",
    "    done=False\n",
    "    observation = env.reset()\n",
    "    new_observation = observation\n",
    "    while done==False:\n",
    "        time.sleep(1/80)\n",
    "        \n",
    "        processed_network_input = preprocess_frames(new_frame=new_observation,last_frame=observation)\n",
    "        reshaped_input = np.expand_dims(processed_network_input,axis=0)\n",
    "\n",
    "        up_probability = policy_network.predict(reshaped_input,batch_size=1)[0][0]\n",
    "        actual_action = np.random.choice(a=[2,3],size=1,p=[up_probability,1-up_probability])\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        observation= new_observation\n",
    "        new_observation, reward, done, info = env.step(actual_action)\n",
    "        if reward!=0:\n",
    "            print(reward)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c665bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode(policy_network_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00cc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2a631",
   "metadata": {},
   "source": [
    "This model losses every game during the test. This is probably caused by the limited training time. We defined more than 1.28 millons paramaters in this model, and it requires huge amount of time to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a440e91",
   "metadata": {},
   "source": [
    "### 1.Establish a baseline performance. How well did your Deep Q-learning do on your problem? (5 Points)\n",
    "<br>\n",
    "train_n_times = 100\n",
    "<br>\n",
    "batch_size = 10\n",
    "<br>\n",
    "learning_rate = 0.7\n",
    "<br>\n",
    "epsilon = 1.\n",
    "<br>\n",
    "max_epsilon = 1.\n",
    "<br>\n",
    "min_epsilon = 0.01\n",
    "<br>\n",
    "decay_rate = 0.05\n",
    "<br>\n",
    "With this baseline setting, the model could not compete with tranditional computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e88734",
   "metadata": {},
   "source": [
    "### 2.What are the states, the actions, and the size of the Q-table? (5 Points)\n",
    "<br>\n",
    "The states are :\n",
    "<br>\n",
    "Box(0, 255, (210, 160, 3), uint8)\n",
    "<br>\n",
    "It is later simplified to (80, 80).\n",
    "<br>\n",
    "The actions are:\n",
    "<br>\n",
    "Discrete(6)\n",
    "<br>\n",
    "Because this model uses Deep Q Learning not Q Learning, there is not Q table. The network involves 2 layers, 200 hidden units, and more than 1.28 millions parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f9412",
   "metadata": {},
   "source": [
    "### 3.What are the rewards? Why did you choose them? (5 Points)\n",
    "<br>\n",
    "The rewards are -1 for lose and 1 for win. The ideal of reinforcement learning is to use reward as a positive reinforce to train the model. Therefore, it is logic to use the game results for rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83473153",
   "metadata": {},
   "source": [
    "### 4.How did you choose alpha and gamma in the Bellman equation? Try at least one additional value for alpha and gamma. How did it change the baseline performance?  (5 Points)\n",
    "<br>\n",
    "In this case I set learning rate as 0.7 and discount as (1-0.7)=0.3. I also examed with (0.8, 0.2). However, because of the time limit, the difference is limited.\n",
    "<br>\n",
    "Theroatically, the learning rate represent the sensitiveness of the model to new training. If the rate is too low, the model learns very slow. Otherwise, if the rate is too high, the model will be domined by only few recent trainings and become very unpredictible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bece27",
   "metadata": {},
   "source": [
    "### 5.Try a policy other than e-greedy. How did it change the baseline performance? (5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57cd8b",
   "metadata": {},
   "source": [
    "### 6.How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode? (5 Points)\n",
    "<br>\n",
    "I set the epsilon to 1 at the start and use 0.05 as decay rate. Then I change the decay rate to 0.01.\n",
    "<br>\n",
    "Therotically, the decay rate influences the stabiliaty of the performence. And as the epsilon decresses the performence should become more stable and, however, unrepresentative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd41c65",
   "metadata": {},
   "source": [
    "### 7.What is the average number of steps taken per episode? (5 Points)\n",
    "<br>\n",
    "Here, because of the bad performance, the steps per episode are very low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c082830",
   "metadata": {},
   "source": [
    "### 8.Does Q-learning use value-based or policy-based iteration? (5 Points)\n",
    "<br>\n",
    "Q-learning is a values-based learning algorithm.\n",
    "<br>\n",
    "Deep Q Learning, on the other hand, is a policy-based algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de577983",
   "metadata": {},
   "source": [
    "### 9.Could you use SARSA for this problem? (5 Points)\n",
    "A SARSA can be used for this problem.\n",
    "SARSA algorithm is an on-policy algorithm. The learning agent learns the value function according to the current action derived from the policy currently being used. The update equation for SARSA depends on the current state, current action, reward obtained, next state and next action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba15ed",
   "metadata": {},
   "source": [
    "### 10.What is meant by the expected lifetime value in the Bellman equation?\n",
    "<br>\n",
    "The expected lifetime value of a state can be decomposed into immediate reward plus the value of successor state with a discount factor. \n",
    "<br>\n",
    "v(t)=R+y*v(t+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404e864",
   "metadata": {},
   "source": [
    "### 11.When would SARSA likely do better than Q-learning? (5 Points)\n",
    "<br>\n",
    "SARSA and Q-Learning are both excellent approaches for reinforcement learning problems. The two approaches work in a finite environment (or a discretized continuous environment).SARSA learns a \"near\" optimal policy while Q-Learning learns the optimal policy directly. Q-Learning is an aggressive agent, while SARSA is a conservative one. For example, walking near the cliff. Q-Learning will take the shortest path as optimal (with the risk of falling), while SARSA will take a longer, safer route (to avoid unexpected falling). Therefore, if the situation involves high risk or cost, SARSA can be a better option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be5a7e",
   "metadata": {},
   "source": [
    "### 12.How does SARSA differ from Q-learning? (5 Points) \n",
    "<br>\n",
    "In Q learning:\n",
    "<br>\n",
    "Q(st,at)←Q(st,at)+α[rt+1+γmaxaQ(st+1,a)−Q(st,at)]\n",
    "<br>\n",
    "SARSA:\n",
    "<br>\n",
    "Q(st,at)←Q(st,at)+α[rt+1+γQ(st+1,at+1)−Q(st,at)]\n",
    "<br>\n",
    "The biggest difference between Q-learning and SARSA is that Q-learning is off-policy, and SARSA is on-policy. SARSA is on-policy because we use the same policy to generate the current action  at  and the next action  at+1. For Q-learning, we have no constraint on how the next action is selected, only that we have this “optimistic” view that all hence-forth action selections from every state should be optimal, thus we pick the action  a  that maximizes  Q(st+1,a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956fdcb",
   "metadata": {},
   "source": [
    "### 13.Explain the Q-learning algorithm. (5 Points) \n",
    "<br>\n",
    "Q-learning is a model-free reinforcement learning algorithm.\n",
    "Q-learning is a values-based learning algorithm. Value based algorithms updates the value function based on an equation(particularly Bellman equation). Whereas the other type, policy-based estimates the value function with a greedy policy obtained from the last policy improvement.\n",
    "Q-learning is an off-policy learner. Means it learns the value of the optimal policy independently of the agent’s actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d730a9e",
   "metadata": {},
   "source": [
    "### 14.Explain the SARSA algorithm. (5 Points) \n",
    "<br>\n",
    "SARSA is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. A SARSA agent interacts with the environment and updates the policy based on actions taken, hence this is known as an on-policy learning algorithm. The Q value for a state-action is updated by an error, adjusted by the learning rate alpha. Q values represent the possible reward received in the next time step for taking action a in state s, plus the discounted future reward received from the next state-action observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998b0ae",
   "metadata": {},
   "source": [
    "## References\n",
    "[1]: Kavinda Kottege - DeepQ-Pong : https://github.com/KavindaKottege/DeepQ-Pong\n",
    "\n",
    "[2]: OpenAI - gym : https://github.com/openai/gym\n",
    "\n",
    "[3]: Wikipedia - State–action–reward–state–action : https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb8fc0",
   "metadata": {},
   "source": [
    "## Licensing\n",
    "\n",
    "Copyright 2022 Yuxin Liu\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615536f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
